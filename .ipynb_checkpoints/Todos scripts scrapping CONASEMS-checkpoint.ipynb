{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5cd89e-9a32-43d5-8aa3-135bb7198474",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 1 scrapping de dados do catálogo virtual de experiências do CONASEMS - salva a página html contendo o catálogo de todas as experiências do site em uma variável\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL da página que você quer copiar o HTML\n",
    "lista_trabalhos_url = \"https://portal.conasems.org.br/brasil-aqui-tem-sus/experiencias?rows=1700&start=1\"\n",
    "\n",
    "# Realiza uma requisição GET para obter o conteúdo da página\n",
    "response = requests.get(lista_trabalhos_url)\n",
    "\n",
    "# Verifica se a requisição foi bem-sucedida\n",
    "if response.status_code == 200:\n",
    "    # Faz o parsing do HTML com BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extrai todo o conteúdo dentro da tag <html> e armazena em uma variável\n",
    "    html_content = str(soup)\n",
    "\n",
    "    # Exibe ou utiliza a variável html_content conforme necessário\n",
    "    print(html_content)\n",
    "else:\n",
    "    print(f\"Erro ao acessar a URL. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e103e0-5774-4abf-81fd-7d1a646a8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 2 scrapping de dados do catálogo virtual de experiências do CONASEMS - extrai informações do catálogo virtual de experiências de html para json e csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "data = []\n",
    "\n",
    "items_experiencias = soup.select(\"#__next > section > section > div\")\n",
    "\n",
    "# Cria um dataframe com as colunas titulo, cidade_estado, autoria, ano e link_experiencia\n",
    "for item in items_experiencias:\n",
    "    infos = item.select_one(\"a > div\").find_all(\"p\")\n",
    "    cidade_estado = infos[0].text\n",
    "    titulo = infos[1].text\n",
    "    autoria = infos[2].text.split(\": \")[1]\n",
    "    ano = infos[3].text\n",
    "    link_experiencia = f\"https://portal.conasems.org.br{item.select_one(\"a\")[\"href\"]}\"\n",
    "    data.append({\n",
    "        \"titulo\": titulo,\n",
    "        \"cidade_estado\": cidade_estado,\n",
    "        \"autoria\": autoria,\n",
    "        \"ano\": ano,\n",
    "        \"link_experiencia\": link_experiencia\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_json(\"lista_itens_CONASEMS.json\", index=False)\n",
    "df.to_csv(\"lista_itens_CONASEMS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99fd8b-8528-4f9c-9722-1184abe32bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 3 scrapping de dados do catálogo virtual de experiências do CONASEM - extrai informações de cada experiência individual (nas páginas específicas das experiências) e integra com o catálogo extraído anteriormente\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o JSON com os links usando pandas\n",
    "lista_itens = pd.read_json('lista_itens_CONASEMS.json')\n",
    "\n",
    "# Lista para armazenar os dados\n",
    "data = []\n",
    "sucessos = []\n",
    "erros = []\n",
    "\n",
    "# Função auxiliar para verificar e extrair o texto\n",
    "def get_element_text(soup, selector):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.get_text(strip=True) if element else \"\"\n",
    "\n",
    "for index, row in lista_itens.iterrows():\n",
    "    url = row[\"link_experiencia\"]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Verificar se a requisição foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extração dos dados usando seletores CSS com :-soup-contains\n",
    "        link = url\n",
    "        coautoria = get_element_text(soup, 'p:-soup-contains(\"Coautor\")')\n",
    "        introducao = get_element_text(soup, 'h3:-soup-contains(\"Apresentação/Introdução\") + p')\n",
    "        objetivos = get_element_text(soup, 'h3:-soup-contains(\"Objetivos\") + p')\n",
    "        metodologia = get_element_text(soup, 'h3:-soup-contains(\"Metodologia\") + p')\n",
    "        resultados = get_element_text(soup, 'h3:-soup-contains(\"Resultados\") + p')\n",
    "        conclusoes = get_element_text(soup, 'h3:-soup-contains(\"Conclusões\") + p')\n",
    "        palavras_chave = get_element_text(soup, 'h3:-soup-contains(\"Palavras-chave\") + p')\n",
    "\n",
    "        # Adicionar dados à lista\n",
    "        data.append({\n",
    "            \"index\": index,\n",
    "            \"link\": link,\n",
    "            \"coautoria\": coautoria, \n",
    "            \"introducao\": introducao, \n",
    "            \"objetivos\": objetivos, \n",
    "            \"metodologia\": metodologia,\n",
    "            \"resultados\": resultados, \n",
    "            \"conclusoes\": conclusoes, \n",
    "            \"palavras_chave\": palavras_chave\n",
    "        })\n",
    "        sucessos.append({index:link})\n",
    "        # Exibir progresso\n",
    "        print(f\"Processando {index + 1}/{len(lista_itens)}\")\n",
    "    else:\n",
    "        erros.append({index:link})\n",
    "        print(f\"Erro ao acessar {index + 1}/{len(lista_itens)} : {response.status_code}\")\n",
    "\n",
    "# Converter os dados para um DataFrame e salvar em CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('resultados_scraping.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb63cf-7ebc-4f99-a7db-61012ad88e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 4 scrapping de dados do catálogo virtual de experiências do CONASEM - unificando os dados do catálogo com os dados das páginas de cada item\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar os arquivos\n",
    "itens = pd.read_json(\"lista_itens_CONASEMS.json\")\n",
    "resumos = pd.read_csv(\"resultados_scraping.csv\")\n",
    "\n",
    "# Renomear a coluna 'link' para 'link_experiencia' em resumos\n",
    "resumos.rename(columns={\"link\": \"link_experiencia\"}, inplace=True)\n",
    "\n",
    "# Fazer o merge mantendo todos os itens de 'itens', mesmo que não estejam em 'resumos'\n",
    "novo_df = pd.merge(itens, resumos, on=\"link_experiencia\", how=\"left\")\n",
    "\n",
    "# Separar 'cidade' e 'estado' da coluna 'cidade_estado'\n",
    "novo_df[['cidade', 'estado']] = novo_df['cidade_estado'].str.split(' - ', expand=True)\n",
    "\n",
    "# Criar o campo 'resumo_completo' incluindo o título e todos os demais campos de interesse\n",
    "novo_df[\"resumo_completo\"] = novo_df[[\"titulo\", \"introducao\", \"objetivos\", \"metodologia\", \"resultados\", \"conclusoes\", \"palavras_chave\"]].fillna('').agg('\\n'.join, axis=1)\n",
    "\n",
    "# Processar a coluna 'coautoria'\n",
    "novo_df['coautoria'] = novo_df['coautoria'].fillna('').apply(lambda x: x.split(':')[1] if len(x.split(':')) > 1 else None).str.upper()\n",
    "\n",
    "# Criar a coluna 'resumo_completo_html' com cabeçalhos HTML para cada seção\n",
    "novo_df['resumo_completo_html'] = (\n",
    "    \"<h1>Título</h1> \" + novo_df['titulo'].fillna('') +\n",
    "    \"<h1>Introdução</h1> \" + novo_df['introducao'].fillna('') +\n",
    "    \"<h1>Objetivos</h1> \" + novo_df['objetivos'].fillna('') +\n",
    "    \"<h1>Metodologia</h1> \" + novo_df['metodologia'].fillna('') +\n",
    "    \"<h1>Resultados</h1> \" + novo_df['resultados'].fillna('') +\n",
    "    \"<h1>Conclusões</h1> \" + novo_df['conclusoes'].fillna('') +\n",
    "    \"<h1>Palavras-chave</h1> \" + novo_df['palavras_chave'].fillna('')\n",
    ")\n",
    "\n",
    "# Adicionar a coluna 'id' que começa de 1\n",
    "novo_df.insert(0, 'id', range(1, len(novo_df) + 1))\n",
    "\n",
    "# Remover a coluna 'index' e reorganizar as colunas na ordem desejada\n",
    "colunas_ordenadas = [\n",
    "    'id', 'titulo', 'cidade', 'estado', 'ano', 'autoria', 'coautoria', 'resumo_completo',\n",
    "    'resumo_completo_html', 'introducao', 'objetivos', 'metodologia', 'resultados', \n",
    "    'conclusoes', 'palavras_chave', 'link_experiencia'\n",
    "]\n",
    "\n",
    "novo_df = novo_df[colunas_ordenadas]\n",
    "\n",
    "# Salvar em diferentes formatos\n",
    "novo_df.to_excel(\"experiencias_completas_CONASEMS.xlsx\", index=False)\n",
    "novo_df.to_csv(\"experiencias_completas_CONASEMS.csv\", index=False)\n",
    "novo_df.to_json(\"experiencias_completas_CONASEMS.json\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
